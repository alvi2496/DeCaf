{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbedder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/C8n85eWlid4MuhANT++n"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm-b0RB9AC7r",
        "colab_type": "text"
      },
      "source": [
        "## Architectural Overview/Design\n",
        "![alt text](https://raw.githubusercontent.com/alvi2496/DeCaf/master/assets/word_embedding.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Up9iaDnZM9l",
        "colab_type": "text"
      },
      "source": [
        "## Objective\n",
        "The main objective is to classify discussions from pull request, issue tracker commit messages and code comments as `design` or `general`. We also want to make the classifier cross project compatible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Av-qUhZOah",
        "colab_type": "text"
      },
      "source": [
        "## Data Collection\n",
        "- We intent to have three types of data. One data is to train the `Word Embedding` model. As `Word Embedding` requires structured form of literature, we have used sentences from literatures(ex. papers and books). Also we have restricted our choice of papers and books to only from the Software Engineering domain for keep the context of our `Word Embedding` model restricted to Software Engineering. Our `Word Embedding` model will be used to vectorize our train data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kJqfFkNeAWNU"
      },
      "source": [
        "## Data Cleaning\n",
        "Raw data can have a lot of noise. Specially when scraped from documents of website, it can contain a lot of misinformation in the form of names, punctuations, numbers(ex. years), misspelled and incompleted words. Also it can have a lot of stopwords that can make the model confused. We have removed all this to make our data as clean as possible. After the cleaning process, out data only contains words that are not stopwords, present in the english dictionary and has lenght greater than three."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A-ie768Gp-2",
        "colab_type": "code",
        "outputId": "8727225e-ae9a-4536-e6f6-05adb47662fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOd6VG5aIqdf",
        "colab_type": "text"
      },
      "source": [
        "## Assign Data location\n",
        "- literature: holds the data for word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9gkg8dEI5fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "literature_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/literature.txt\"\n",
        "enhanced_literature_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/enhanced_literature.txt\"\n",
        "so_model_file = \"/content/drive/My Drive/documents/projects/DeCaf/models/pre_trained/so.bin\"\n",
        "we_model_file = \"/content/drive/My Drive/documents/projects/DeCaf/models/we.bin\"\n",
        "enhanced_we_model_file = \"/content/drive/My Drive/documents/projects/DeCaf/models/enhanced_we.bin\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa6lcx4ePsEM",
        "colab_type": "text"
      },
      "source": [
        "## Enhance the Literature\n",
        "- We enhance the `literature` using pre-trained model `so.bin`\n",
        "  - We are taking every word of literature\n",
        "  - Inject additional similar word related to software engineering to enhance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF2d3MNeblyF",
        "colab_type": "text"
      },
      "source": [
        "### Read the literature.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLcD_v0AbpX_",
        "colab_type": "code",
        "outputId": "7f89eb9a-ee14-4530-dbae-e2ae9f520664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "literature = open(literature_file, 'r')\n",
        "words = literature.read().split(\" \")\n",
        "literature.close()\n",
        "print(\"Total words in literature: \", len(words)) "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in literature:  1575439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FzeUGXLEg-K",
        "colab_type": "text"
      },
      "source": [
        "### Divide the words in chunks\n",
        "- We divide the words in chunks to implement multiprocessing on each chunks\n",
        "- We take 300000 words in each chunks leading to 5 chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Hx6AT9E7XR",
        "colab_type": "code",
        "outputId": "023567ca-7664-4652-c270-6f60e68ae39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "file_chunks = []\n",
        "file_chunks_names = []\n",
        "index = 0\n",
        "while index < len(words) - 1:\n",
        "  chunk = []\n",
        "  lower_limit = index\n",
        "  i = 0\n",
        "  while i < 4 and lower_limit < len(words) - 1:\n",
        "    upper_limit = lower_limit + 75000\n",
        "    if upper_limit > len(words):\n",
        "      upper_limit = len(words) - 1\n",
        "    chunk.append(words[lower_limit:upper_limit])\n",
        "    print(\"lower_limit: \" + str(lower_limit) + \" upper_limit: \" + str(upper_limit))\n",
        "    lower_limit = upper_limit\n",
        "    i = i + 1\n",
        "  file_chunks_names.append(str(index) + '_' + str(lower_limit))\n",
        "  index = lower_limit\n",
        "  file_chunks.append(chunk)\n",
        "print(file_chunks_names)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lower_limit: 0 upper_limit: 75000\n",
            "lower_limit: 75000 upper_limit: 150000\n",
            "lower_limit: 150000 upper_limit: 225000\n",
            "lower_limit: 225000 upper_limit: 300000\n",
            "lower_limit: 300000 upper_limit: 375000\n",
            "lower_limit: 375000 upper_limit: 450000\n",
            "lower_limit: 450000 upper_limit: 525000\n",
            "lower_limit: 525000 upper_limit: 600000\n",
            "lower_limit: 600000 upper_limit: 675000\n",
            "lower_limit: 675000 upper_limit: 750000\n",
            "lower_limit: 750000 upper_limit: 825000\n",
            "lower_limit: 825000 upper_limit: 900000\n",
            "lower_limit: 900000 upper_limit: 975000\n",
            "lower_limit: 975000 upper_limit: 1050000\n",
            "lower_limit: 1050000 upper_limit: 1125000\n",
            "lower_limit: 1125000 upper_limit: 1200000\n",
            "lower_limit: 1200000 upper_limit: 1275000\n",
            "lower_limit: 1275000 upper_limit: 1350000\n",
            "lower_limit: 1350000 upper_limit: 1425000\n",
            "lower_limit: 1425000 upper_limit: 1500000\n",
            "lower_limit: 1500000 upper_limit: 1575000\n",
            "lower_limit: 1575000 upper_limit: 1575438\n",
            "['0_300000', '300000_600000', '600000_900000', '900000_1200000', '1200000_1500000', '1500000_1575438']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66vRsHcKAWyF",
        "colab_type": "text"
      },
      "source": [
        "### Import the so word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHGyO9D9QZC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime as dt\n",
        "\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "so_model = KeyedVectors.load_word2vec_format(so_model_file, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PvZAqO8k4lj",
        "colab_type": "text"
      },
      "source": [
        "### Inject similar words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JaSUKqccetw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inject_similar_words(process_number, words, injected_word_chunks):\n",
        "  start_time = dt.now()\n",
        "  for i in range(len(words)):\n",
        "    try:\n",
        "      words_to_inject = [words[i]]\n",
        "      similar_words = so_model.most_similar(words[i])\n",
        "      for similar_tuple in similar_words:\n",
        "        if similar_tuple[1] > 0.5:\n",
        "          words_to_inject.append(similar_tuple[0])\n",
        "      words[i] = \" \".join(words_to_inject)\n",
        "    except:\n",
        "      continue\n",
        "    if i % 10000 == 0:\n",
        "      t = dt.now() - start_time\n",
        "      start_time = dt.now()\n",
        "      print(\"CPU \" + str(process_number) + \": Processed: \" + str(i) + \" / \" + str(len(words)) + \" words in time: \", t)\n",
        "  injected_word_chunks[process_number] = words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Oaai2WHZTZ",
        "colab_type": "code",
        "outputId": "cbb68d0f-5513-430e-d8cf-c6b7771b0e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "print(\"Available CPUs: \", mp.cpu_count())\n",
        "index = 0\n",
        "for word_chunks in file_chunks:\n",
        "  literature_chunk_name = \"/content/drive/My Drive/documents/projects/DeCaf/data/literature_chunks/\" + file_chunks_names[index] + \".txt\"\n",
        "  if not os.path.exists(literature_chunk_name):\n",
        "    print(\"Working on chunk: \", file_chunks_names[index])\n",
        "    manager = mp.Manager()\n",
        "    injected_word_chunks = manager.dict()\n",
        "    jobs = []\n",
        "\n",
        "    for i in range(len(word_chunks)):\n",
        "      p = mp.Process(target=inject_similar_words, args=(i, word_chunks[i], injected_word_chunks))\n",
        "      jobs.append(p)\n",
        "      p.start()\n",
        "\n",
        "    for proc in jobs:\n",
        "      proc.join()\n",
        "\n",
        "    injected_word_chunks = injected_word_chunks.values()\n",
        "\n",
        "    corpus = []\n",
        "    for chunk in injected_word_chunks:\n",
        "      corpus.append(\" \".join(chunk))\n",
        "    corpus = \" \".join(corpus)\n",
        "    literature = open(literature_chunk_name, 'w')\n",
        "    literature.write(corpus)\n",
        "    literature.close()\n",
        "  index = index + 1\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available CPUs:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnuWn5XkCjfO",
        "colab_type": "text"
      },
      "source": [
        "### Concatenate and save the chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QrFDKRdCnOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(enhanced_literature_file):\n",
        "  chunks_directory = \"/content/drive/My Drive/documents/projects/DeCaf/data/literature_chunks/\"\n",
        "  print(chunks_names)\n",
        "  with open(enhanced_literature_file, 'w') as outfile:\n",
        "    for names in chunks_names:\n",
        "      with open(chunks_directory + names) as infile:\n",
        "        outfile.write(infile.read())\n",
        "      outfile.write(\" \")\n",
        "  print('File created!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnD4SBPUJSir",
        "colab_type": "text"
      },
      "source": [
        "## Create Word Embedding\n",
        "- Use fasttext for word embedding\n",
        "  - we take literature.txt as our input data.\n",
        "  - we train the classifier upsupervised since we just want to group the data according to the similarity.\n",
        "  - we have used skipgram as we have observe that skipgram models works better with subword information that cbow\n",
        "  - we are taking words with character number from 4-20. since we are removing every word less than three characters, its not important to take the characters less than 4 characters. Also, the design words seems to be on the bigger side. for ex. reproduceability contains 16 characters. we are considering characters upto 25 characters.\n",
        "  - We are taking 300 dimension of the words. Also looping for 10 epochs. Both because the training corpus is relatively small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8LPXDN4KJMH",
        "colab_type": "code",
        "outputId": "85ab2572-c29b-449e-98cf-87c29542d47f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.0.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZ5xYWTbXoJ",
        "colab_type": "text"
      },
      "source": [
        "## Create Logging Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8H39Gum_Jy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def log(log_file_path, message):\n",
        "  file = pd.DataFrame([[str(dt.now()), 'info', message]], columns=['timestamp', 'type', 'message'])\n",
        "  if not os.path.exists(log_file_path):\n",
        "    file.to_csv(log_file_path)\n",
        "  else:\n",
        "    file.to_csv(log_file_path, mode='a', header=False)\n",
        "\n",
        "def log_result(log_file_path, message):\n",
        "  f = open(log_file_path, \"a\")\n",
        "  f.write(str(dt.now()))\n",
        "  f.write(message)\n",
        "  f.close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9GxG-n5SwdC",
        "colab_type": "text"
      },
      "source": [
        "## Train the `Word Embedding` model and save it as we.bin\n",
        "- Train and save the model if the model is not present\n",
        "- Load the model from disk if preselt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHxShLcsFoa",
        "colab_type": "code",
        "outputId": "cda9c7c7-88a4-4bda-d6a0-b372ff4896d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import fasttext as ft\n",
        "\n",
        "log_file_path = \"/content/drive/My Drive/documents/projects/DeCaf/logs/we.csv\"\n",
        "we_log = []\n",
        "\n",
        "if not os.path.exists(enhanced_we_model_file):\n",
        "  print(str(dt.now())+' Training Word Embedding model...')\n",
        "  we_log.append(str(dt.now())+' Training Word Embedding model...')\n",
        "  we_model = ft.train_unsupervised(enhanced_we_model_file, \"skipgram\", minn=4, \\\n",
        "                                   maxn=25, dim=200, epoch=10)\n",
        "  we_model.save_model(enhanced_we_model_file)\n",
        "  print(str(dt.now())+' Model trained and saved successfully')\n",
        "  we_log.append(str(dt.now())+' Model trained and saved successfully')\n",
        "else:\n",
        "  print(str(dt.now())+' Loading Word Embedding model from disk...')\n",
        "  we_log.append(str(dt.now())+' Loading Word Embedding model from disk...')\n",
        "  we_model = ft.load_model(enhanced_we_model_file)\n",
        "  print(str(dt.now())+' Model loaded successfully.')\n",
        "  we_log.append(str(dt.now())+' Model loaded successfully.')\n",
        "\n",
        "log(log_file_path, \\\n",
        "    \"\\n\".join(we_log))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 07:46:14.202459 Loading Word Embedding model from disk...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 07:46:46.632964 Model loaded successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ezQOGUS9JO",
        "colab_type": "text"
      },
      "source": [
        "## Playing around with the Word Embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgToIpCwXp5F",
        "colab_type": "text"
      },
      "source": [
        "Get the dimention of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XgUwC1mXsxb",
        "colab_type": "code",
        "outputId": "f337cb52-63f5-447d-f18e-1554dc5f47f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "we_model.get_dimension()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzgaNhGTFeV",
        "colab_type": "text"
      },
      "source": [
        "Get the words of the model... For a demo... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqgszQ6rXBmk",
        "colab_type": "code",
        "outputId": "148c554d-2928-4365-941c-74e880a192bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "we_model.get_words()[0:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['systems',\n",
              " 'applications',\n",
              " 'software',\n",
              " 'therefore',\n",
              " 'programs',\n",
              " 'objects',\n",
              " 'softwares',\n",
              " 'avionics',\n",
              " 'hardware',\n",
              " 'since']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfDXAMsiXizi",
        "colab_type": "text"
      },
      "source": [
        "Get the vector of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwQIRXXdXWJL",
        "colab_type": "code",
        "outputId": "b6530270-4614-4db0-828e-843818cf7195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "we_model.get_word_vector('Maintainability')[0:20]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.221413  , -0.01180398,  0.00891189, -0.15669821,  0.01679295,\n",
              "        0.06836811, -0.25612646,  0.48448488,  0.24435401, -0.35936418,\n",
              "        0.23399256,  0.17508   ,  0.56289315, -0.00411335, -0.3178893 ,\n",
              "       -0.5048857 ,  0.21113715, -0.22008029, -0.783839  ,  0.01447019],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmzrg38PXzxt",
        "colab_type": "text"
      },
      "source": [
        "Get the vector of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otivEp_7X3I-",
        "colab_type": "code",
        "outputId": "5d5730ba-a9b5-46f0-f356-b87969f32715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vector = we_model.get_sentence_vector(\"Add performance tracker to active admin jobs\")\n",
        "print(vector.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}