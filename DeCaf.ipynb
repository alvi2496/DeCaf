{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeCaf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue-brQOCOR4A",
        "colab_type": "text"
      },
      "source": [
        "# Notebook for the experiment of building **DeCaf** (**De**sign **C**l**a**ssi**f**ier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_wliyuuasNx",
        "colab_type": "text"
      },
      "source": [
        "## Architectural Overview/Design\n",
        "![alt text](https://raw.githubusercontent.com/alvi2496/DeCaf/master/assets/architectural_overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Up9iaDnZM9l",
        "colab_type": "text"
      },
      "source": [
        "## Objective\n",
        "The main objective is to classify discussions from pull request, issue tracker commit messages and code comments as `design` or `general`. We also want to make the classifier cross project compatible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Av-qUhZOah",
        "colab_type": "text"
      },
      "source": [
        "## Data Collection\n",
        "- We intent to have three types of data. One data is to train the `Word Embedding` model. As `Word Embedding` requires structured form of literature, we have used sentences from literatures(ex. papers and books). Also we have restricted our choice of papers and books to only from the Software Engineering domain for keep the context of our `Word Embedding` model restricted to Software Engineering. Our `Word Embedding` model will be used to vectorize our train data.\n",
        "- We have collected our train data from Stack Overflow questions, answers and comments. We have collected data and classified them as `design` or `general` based on the tag. For example, questions and answers that contain `design-patterns` or `software-design` falls under the class of `design` while data tagged as `javascript` or `django` as classified as `general`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2xxEU72ZPf_",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning\n",
        "Raw data can have a lot of noise. Specially when scraped from documents of website, it can contain a lot of misinformation in the form of names, punctuations, numbers(ex. years), misspelled and incompleted words. Also it can have a lot of stopwords that can make the model confused. We have removed all this to make our data as clean as possible. After the cleaning process, out data only contains words that are not stopwords, present in the english dictionary and has lenght greater than three."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A-ie768Gp-2",
        "colab_type": "code",
        "outputId": "6cdf80dc-9a3d-4e41-a295-3deceb83d035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOd6VG5aIqdf",
        "colab_type": "text"
      },
      "source": [
        "## Assign Data location\n",
        "- literature: holds the data for word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9gkg8dEI5fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "literature_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/literature.txt\"\n",
        "we_model_file = \"/content/drive/My Drive/documents/projects/DeCaf/models/we.bin\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnD4SBPUJSir",
        "colab_type": "text"
      },
      "source": [
        "## Create Word Embedding\n",
        "- Use fasttext for word embedding\n",
        "1. we take literature.txt as our input data.\n",
        "2. we train the classifier upsupervised since we just want to group the data according to the similarity.\n",
        "3. we have used skipgram as we have observe that skipgram models works better with subword information that cbow\n",
        "4. we are taking words with character number from 4-20. since we are removing every word less than three characters, its not important to take the characters less than 4 characters. Also, the design words seems to be on the bigger side. for ex. reproduceability contains 16 characters. we are considering characters upto 25 characters.\n",
        "5. We are taking 300 dimension of the words. Also looping for 10 epochs. Both because the training corpus is relatively small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8LPXDN4KJMH",
        "colab_type": "code",
        "outputId": "8f4300eb-94c6-412f-e3f3-fac6df586623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9GxG-n5SwdC",
        "colab_type": "text"
      },
      "source": [
        "## Train the `Word Embedding` model and save it as we.bin\n",
        "- Train and save the model if the model is not present\n",
        "- Load the model from disk if preselt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8H39Gum_Jy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime as dt\n",
        "\n",
        "def log(log_file_path, message):\n",
        "  file = pd.DataFrame([[str(dt.now()), 'info', message]], columns=['timestamp', 'type', 'message'])\n",
        "  if not os.path.exists(log_file_path):\n",
        "    file.to_csv(log_file_path)\n",
        "  else:\n",
        "    file.to_csv(log_file_path, mode='a', header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHxShLcsFoa",
        "colab_type": "code",
        "outputId": "f93536f5-02b9-40c7-b78d-afbdf98547f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import os\n",
        "import fasttext as ft\n",
        "\n",
        "log_file_path = \"/content/drive/My Drive/documents/projects/DeCaf/logs/we.csv\"\n",
        "we_log = []\n",
        "\n",
        "if not os.path.exists(we_model_file):\n",
        "  print(str(dt.now())+' Training Word Embedding model...')\n",
        "  we_log.append(str(dt.now())+' Training Word Embedding model...')\n",
        "  we_model = ft.train_unsupervised(literature_file, \"skipgram\", minn=4, \\\n",
        "                                   maxn=25, dim=300, epoch=10)\n",
        "  we_model.save_model(we_model_file)\n",
        "  print(str(dt.now())+' Model trained and saved successfully')\n",
        "  we_log.append(str(dt.now())+' Model trained and saved successfully')\n",
        "else:\n",
        "  print(str(dt.now())+' Loading Word Embedding model from disk...')\n",
        "  we_log.append(str(dt.now())+' Loading Word Embedding model from disk...')\n",
        "  we_model = ft.load_model(we_model_file)\n",
        "  print(str(dt.now())+' Model loaded successfully.')\n",
        "  we_log.append(str(dt.now())+' Model loaded successfully.')\n",
        "\n",
        "log(\"/content/drive/My Drive/documents/projects/DeCaf/logs/we.csv\", \\\n",
        "    \"\\n\".join(we_log))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-31 01:32:26.448169 Loading Word Embedding model from disk...\n",
            "2020-03-31 01:32:31.906517 Model loaded successfully.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ezQOGUS9JO",
        "colab_type": "text"
      },
      "source": [
        "## Playing around with the Word Embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgToIpCwXp5F",
        "colab_type": "text"
      },
      "source": [
        "Get the dimention of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XgUwC1mXsxb",
        "colab_type": "code",
        "outputId": "fe979545-3237-4a42-cace-94afea53d4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "we_model.get_dimension()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzgaNhGTFeV",
        "colab_type": "text"
      },
      "source": [
        "Get the words of the model... For a demo... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqgszQ6rXBmk",
        "colab_type": "code",
        "outputId": "045e3620-de2b-4bc7-ef11-c63c89584047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "we_model.get_words()[0:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['software',\n",
              " 'design',\n",
              " 'system',\n",
              " 'data',\n",
              " 'development',\n",
              " 'process',\n",
              " 'code',\n",
              " 'model',\n",
              " 'used',\n",
              " 'systems']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfDXAMsiXizi",
        "colab_type": "text"
      },
      "source": [
        "Get the vector of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwQIRXXdXWJL",
        "colab_type": "code",
        "outputId": "83d79edc-6ec4-485a-c5fc-ac4fe9c4159c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "we_model.get_word_vector('Maintainability')[0:20]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.33910003, -0.02462115,  0.2564945 ,  0.20370075,  0.24550036,\n",
              "       -0.08598559, -0.19060151, -0.24320789, -0.31477034,  0.20625925,\n",
              "       -0.157274  ,  0.05560962,  0.06384223,  0.15963511,  0.08018538,\n",
              "        0.33532   , -0.03695407,  0.11110429,  0.11759775,  0.03177561],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmzrg38PXzxt",
        "colab_type": "text"
      },
      "source": [
        "Get the vector of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otivEp_7X3I-",
        "colab_type": "code",
        "outputId": "3bdb6aa9-49ac-4c8a-ad40-ef9663c3b612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vector = we_model.get_sentence_vector(\"Add performance tracker to active admin jobs\")\n",
        "print(vector.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND34XuJDLy2F",
        "colab_type": "text"
      },
      "source": [
        "## Read and process **TRAIN** data\n",
        "- We use three types of data from training the model. We scraped question, answer and comment data from Stack Overflow and tagged them `design` or `general`. The tagging was done automatically based on the original tag of the question. Then we process our data as we did before for our word embedding data to clear noise. We also did some additional processing to our train data. We only took those documents that contains more than 10 words. For the others, we discarded them. After processing, we got 1,00,000 documents(50,000-design, 50,000-general) for `questions.csv`, 40,000 documents(20,000-design, 20,000-general) for `answers.csv` and 60,000 documents(30,000-design, 30,000-general) for `comments.csv`.\n",
        "\n",
        "- Our train data is completely noise free, stopwords free and long documents of more than 10 words per document. All the data is randomly distributed.\n",
        "\n",
        "- After processing the data and converting the data in vector with the help of our trained word embedding model, we save the data as .npy file for quick access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E30pMihMDCA",
        "colab_type": "text"
      },
      "source": [
        "#### Assign train data location\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGy3g1XTOC2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_question_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/questions.csv\"\n",
        "train_answer_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/answers.csv\"\n",
        "train_comment_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/comments.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjduBEkLOjcN",
        "colab_type": "text"
      },
      "source": [
        "#### Read the data with pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw_-kkRROo5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "train_question = pd.read_csv(train_question_file)\n",
        "train_answer = pd.read_csv(train_answer_file)\n",
        "train_comment = pd.read_csv(train_comment_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NazCrfAzPD0J",
        "colab_type": "text"
      },
      "source": [
        "#### Explore the train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbwPwlRvT2oJ",
        "colab_type": "code",
        "outputId": "db4c59fe-ac42-4b60-e009-07444936f79a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "print(\"Question train data\")\n",
        "print(train_question.head())\n",
        "print(\"Answer train data\")\n",
        "print(train_answer.head())\n",
        "print(\"Comment train data\")\n",
        "print(train_comment.head())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question train data\n",
            "                                            question    label\n",
            "0  control pattern according principle elsewhere ...   design\n",
            "1  messenger throws error getting errors messenge...  general\n",
            "2  error cannot find symbol variable trying integ...   design\n",
            "3  data mapper pattern different repository patte...   design\n",
            "4  start project using poetry start project using...  general\n",
            "Answer train data\n",
            "                                              answer    label\n",
            "0  just tell solved issue found feed posted file ...  general\n",
            "1  does actually implement copy cannot implement ...  general\n",
            "2  following comments included code note will spe...  general\n",
            "3  looks user present undefined method error need...  general\n",
            "4  think better natural argue method names explic...   design\n",
            "Comment train data\n",
            "                                             comment    label\n",
            "0  public declared class later library members sa...   design\n",
            "1  using server definitely needs reasonable chanc...  general\n",
            "2  defined crossproduct read rust programming lan...  general\n",
            "3  nice just clear means calling route times data...   design\n",
            "4  really difficult understand requirements examp...   design\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZR0EwyIPOvU",
        "colab_type": "code",
        "outputId": "7728ec0d-d727-49da-da8a-436bfb645896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = [\"Dataset Name\", \"Shape\", \"# of design data\", \"# of general data\"]\n",
        "\n",
        "qd = train_question[train_question['label']=='design']\n",
        "qg = train_question[train_question['label']=='general']\n",
        "\n",
        "ad = train_answer[train_answer['label']=='design']\n",
        "ag = train_answer[train_answer['label']=='general']\n",
        "\n",
        "cd = train_comment[train_comment['label']=='design']\n",
        "cg = train_comment[train_comment['label']=='general']\n",
        "\n",
        "table.add_row([\"Question\", train_question.shape, qd.shape[0], qg.shape[0]])\n",
        "table.add_row([\"Answer\", train_answer.shape, ad.shape[0], ag.shape[0]])\n",
        "table.add_row([\"Comment\", train_comment.shape, cd.shape[0], cg.shape[0]])\n",
        "\n",
        "\n",
        "print(table)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+-------------+------------------+-------------------+\n",
            "| Dataset Name |    Shape    | # of design data | # of general data |\n",
            "+--------------+-------------+------------------+-------------------+\n",
            "|   Question   | (100000, 2) |      50000       |       50000       |\n",
            "|    Answer    |  (40000, 2) |      20000       |       20000       |\n",
            "|   Comment    |  (60000, 2) |      30000       |       30000       |\n",
            "+--------------+-------------+------------------+-------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ntcn6aKPE3",
        "colab_type": "text"
      },
      "source": [
        "### Convert sentences in vector uisng Word Embedding model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q52NSKvjeoQA",
        "colab_type": "text"
      },
      "source": [
        "Now we turn our head towards converting our train data to vectors of 300 dimension. We are naming our variable by the following convension:\n",
        "- question data = X_Q, question label = Y_Q\n",
        "- answer data = X_A, answer label = Y_A\n",
        "- comment data = X_C, comment label = Y_C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TmaOl6StWKM",
        "colab_type": "text"
      },
      "source": [
        "#### Location to save/retrive data in vector format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz5I9xV8tkEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Q_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/X_Q.npy\"\n",
        "Y_Q_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/Y_Q.npy\"\n",
        "\n",
        "X_A_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/X_A.npy\"\n",
        "Y_A_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/Y_A.npy\"\n",
        "\n",
        "X_C_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/X_C.npy\"\n",
        "Y_C_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/Y_C.npy\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q47D7-6yVIr",
        "colab_type": "text"
      },
      "source": [
        "We define `get_text_vector(data, url)` function to convert text sentences into 300 dimension word vector. We define `get_label_vector(label, url)` to convert the labels into vector.\n",
        "- look for the .npy file in the disk. if found then load the data into memory.\n",
        "- if not found then \n",
        "  - convert the text and labels into vector\n",
        "  - save the text and labels as .npy file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDBnIkvafAV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_text_vector(data, data_url):\n",
        "  if os.path.exists(data_url):\n",
        "    X = np.load(data_url)\n",
        "  else:\n",
        "    X = []\n",
        "    for sentence in data:\n",
        "      X.append(we_model.get_sentence_vector(sentence))\n",
        "    X = np.array(X)\n",
        "    np.save(data_url, X)\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpb4wbAJgYrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label_vector(labels, label_url):\n",
        "  if os.path.exists(label_url):\n",
        "    Y = np.load(label_url)\n",
        "  else:\n",
        "    Y = []\n",
        "    for label in labels:\n",
        "      if label == 'design':\n",
        "        Y.append(1)\n",
        "      else:\n",
        "        Y.append(0)\n",
        "    Y = np.array(Y)\n",
        "    np.save(label_url, Y)\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-7wCNmayK9i",
        "colab_type": "text"
      },
      "source": [
        "#### Load and inspect the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lo1ihPqbyCk",
        "colab_type": "code",
        "outputId": "d4bbd525-2a9e-4b87-d84d-cd5fd5b987ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "X_Q = get_text_vector(train_question['question'], X_Q_url)\n",
        "Y_Q = get_label_vector(train_question['label'], Y_Q_url)\n",
        "print('Shape of X_Q: ', X_Q.shape)\n",
        "print('Shape of Y_Q: ', Y_Q.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_Q:  (100000, 300)\n",
            "Shape of Y_Q:  (100000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NjL4tUcdb8z",
        "colab_type": "code",
        "outputId": "4aaafb23-4ade-4fe1-dbd2-d7a8574ea843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "X_A = get_text_vector(train_answer['answer'], X_A_url)\n",
        "Y_A = get_label_vector(train_answer['label'], Y_A_url)\n",
        "print('Shape of X_A: ', X_A.shape)\n",
        "print('Shape of Y_A: ', Y_A.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_A:  (40000, 300)\n",
            "Shape of Y_A:  (40000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2-zNJMnspD0",
        "colab_type": "code",
        "outputId": "60af295b-e3b5-4a4d-ecf5-cd5e0835393c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "X_C = get_text_vector(train_comment['comment'], X_C_url)\n",
        "Y_C = get_label_vector(train_comment['label'], Y_C_url)\n",
        "print('Shape of X_C: ', X_C.shape)\n",
        "print('Shape of Y_C: ', Y_C.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_C:  (60000, 300)\n",
            "Shape of Y_C:  (60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdvw7gNIVSEf",
        "colab_type": "text"
      },
      "source": [
        "## Read and process **VALIDATION** data\n",
        "- We are taking the same approach as above to read, process, convert and save our validation data.\n",
        "- Validation data contains a mixture of `question`, `answer` and `comment` data\n",
        "- X_V for the vector of the text, Y_V is the vector of the labels for validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNeKSdPtV9Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/validation_data/validation.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y0abI0aWMCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = pd.read_csv(validation_data_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l240dbRLWXGe",
        "colab_type": "code",
        "outputId": "d9d66e15-2887-4edc-867d-c93e8f0b5568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(\"Shape: \", validation_data.shape)\n",
        "print(validation_data.head())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape:  (30000, 2)\n",
            "                                                text    label\n",
            "0  better handling type issue sort development us...  general\n",
            "1  custom route dependency injection route define...   design\n",
            "2  crawler design calling async calling service l...   design\n",
            "3  page seems freeze triggered event issue based ...  general\n",
            "4  install package composer require trying compos...  general\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuZkcU_aWn5t",
        "colab_type": "code",
        "outputId": "58f5f377-9d3a-4983-d995-48b1eec81b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Number of design data: \", validation_data[validation_data[\"label\"] == \"design\"].shape[0])\n",
        "print(\"number of general data: \", validation_data[validation_data[\"label\"] == \"general\"].shape[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of design data:  15000\n",
            "number of general data:  15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmQ45P1QXOnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_V = get_text_vector(validation_data['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/validation_data/X_V.npy\")\n",
        "Y_V = get_label_vector(validation_data['label'], \"/content/drive/My Drive/documents/projects/DeCaf/data/validation_data/Y_V.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grhes7JkX2ql",
        "colab_type": "code",
        "outputId": "de3afe9b-d102-48ca-b475-afc01ca0c53c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Shape of X_V: \", X_V.shape)\n",
        "print(\"Shape of Y_V: \", Y_V.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_V:  (30000, 300)\n",
            "Shape of Y_V:  (30000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv7WbsJpkquT",
        "colab_type": "text"
      },
      "source": [
        "## Read and process **TEST** data\n",
        "- We are taking the same approach as above to read, process, convert and save our validation data.\n",
        "- Test data contains a mixture of `question`, `answer` and `comment` data\n",
        "- X_T for the vector of the text, Y_T is the vector of the labels for validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "293UpZ0SkzhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/test_data/test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qddbU7FHk9MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(test_data_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuJEcM1DlFVH",
        "colab_type": "code",
        "outputId": "d637d899-9a1e-4518-9731-be70c07c6123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(\"Shape: \", test_data.shape)\n",
        "print(test_data.head())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape:  (30000, 2)\n",
            "                                                text    label\n",
            "0  form using submit insert fairly working stuck ...  general\n",
            "1  using shared drive portal storage sharing clie...   design\n",
            "2  scale parallel project looking using together ...   design\n",
            "3  decouple service associated database separate ...   design\n",
            "4  method does exist trying send mail user know s...  general\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S-jB0F9lNaW",
        "colab_type": "code",
        "outputId": "65eb665e-577e-4cae-c6b1-35db42b69e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Number of design data: \", test_data[test_data[\"label\"] == \"design\"].shape[0])\n",
        "print(\"number of general data: \", test_data[test_data[\"label\"] == \"general\"].shape[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of design data:  15000\n",
            "number of general data:  15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHrhJIDolUO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_T = get_text_vector(test_data['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/test_data/X_T.npy\")\n",
        "Y_T = get_label_vector(test_data['label'], \"/content/drive/My Drive/documents/projects/DeCaf/data/test_data/Y_T.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y4OAmiklsIM",
        "colab_type": "code",
        "outputId": "c6641631-3607-4125-9ae0-c3b445d3ed82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Shape of X_T: \", X_T.shape)\n",
        "print(\"Shape of Y_T: \", Y_T.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_T:  (30000, 300)\n",
            "Shape of Y_T:  (30000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyL6iWCiNlwR",
        "colab_type": "text"
      },
      "source": [
        "## Experiment with Traditional Data Mining Algorithms\n",
        "- Before start experimenting with deep learning, we start our experiment with training some traditional data mining algorithms. We are taking the following classifiers with notations and parameter configurations:\n",
        "  - k-Nearest Neighbors (`knn`)\n",
        "  - Decision Tree (`dt`)\n",
        "  - Random Forest (`rf`)\n",
        "  - Logistic Regression (`lr`)\n",
        "  - Linear SVM (`lsvm`)\n",
        "    - C = Regularization Parameter\n",
        "  - RBF SVM (`rbf_svm`)\n",
        "    - Kernel coefficient = 2 \n",
        "    - Regularization parameter = default = 1\n",
        "  - Neural Net (`nn`)\n",
        "  - AdaBoost (`ab`)\n",
        "  - Naive Bayes (`gnb`)\n",
        "  - QDA (`qda`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-7yQzbVCW7",
        "colab_type": "text"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KML4RtuuRum4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq7d0YOLVF6K",
        "colab_type": "text"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsab_DKqQAak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "classifier_names = [\n",
        "      \"Nearest Neighbors\",\n",
        "      \"Decision Tree\",\n",
        "      \"Random Forest\",\n",
        "      \"Logistic Regression\",\n",
        "      \"Gaussian Naive Bayes\", \n",
        "      \"Neural Net\", \n",
        "      \"AdaBoost\",\n",
        "      \"QDA\",    \n",
        "      \"Linear SVM\", \n",
        "      \"RBF SVM\",\n",
        "]\n",
        "\n",
        "model_paths = [\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/knn.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/dt.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/rf.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/lr.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/gnb.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/nn.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/ab.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/qda.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/lsvm.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/rbf_svm.joblib\"\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "      KNeighborsClassifier(n_jobs=-1),\n",
        "      DecisionTreeClassifier(), \n",
        "      RandomForestClassifier(n_jobs=-1), \n",
        "      LogisticRegression(max_iter=50000),\n",
        "      GaussianNB(),\n",
        "      MLPClassifier(max_iter=4000),\n",
        "      AdaBoostClassifier(),\n",
        "      QuadraticDiscriminantAnalysis(),\n",
        "      SVC(kernel=\"linear\", C=0.025), \n",
        "      SVC(gamma=2, C=1)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Q0fquUXt8c",
        "colab_type": "text"
      },
      "source": [
        "### Combine the three train data namely: `questions`, `answers` and `comment`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj2e6MSCX9Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.concatenate((X_Q, X_A, X_C), axis=0)\n",
        "Y = np.concatenate((Y_Q, Y_A, Y_C), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TozZfIXeYaw2",
        "colab_type": "text"
      },
      "source": [
        "### Examine X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbxIWifrYfYS",
        "colab_type": "code",
        "outputId": "f4463157-c46b-415b-a713-1d4c594fdd94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200000, 300)\n",
            "(200000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kV5XMI7PYOP",
        "colab_type": "text"
      },
      "source": [
        "### **Train** and **Save** the models into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQswNls3VuOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "log = []\n",
        "\n",
        "for index, classifier in enumerate(classifiers):\n",
        "  if not os.path.exists(model_paths[index]):\n",
        "    start_time = dt.now()\n",
        "    print(str(start_time) + \" Started training model: \", classifier_names[index])\n",
        "    log.append(str(start_time) + \" Started training model: \", classifier_names[index])\n",
        "    model = classifier.fit(X, Y)\n",
        "    end_time = dt.now()\n",
        "    print(str(end_time) + \" Finished training model: \", classifier_names[index])\n",
        "    log.append(str(end_time) + \" Finished training model: \", classifier_names[index])\n",
        "    print(\"Time to train model: \", end_time - start_time)\n",
        "    log.append(\"Time to train model: \", end_time - start_time)\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    dump(model, model_paths[index])\n",
        "    log(\"/content/drive/My Drive/documents/projects/DeCaf/logs/model_train.csv\", \"\\n\".join(log))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljPxEs-4Ngx0",
        "colab_type": "text"
      },
      "source": [
        "### Validate the models\n",
        "- We use Area Under the Receiver Operating Characteristic Curve (**ROC AUC**) from prediction scores as the validation criteria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLxh9uyZN4Kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def auc_score(X, Y, model):\n",
        "  pred = model.predict(X)\n",
        "  auc = metrics.roc_auc_score(Y, pred)\n",
        "  return auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQKN9WbuOIus",
        "colab_type": "text"
      },
      "source": [
        "#### Load the models and calculate the **ROC AUC** score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Fj_9TWOREB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = classifier_names\n",
        "log = []\n",
        "auc_scores = []\n",
        "for index, model_path in enumerate(model_paths):\n",
        "  start_loading_time = dt.now()\n",
        "  print(str(start_loading_time) + \" Loading model: \", classifier_names[index])\n",
        "  log.append(str(start_loading_time) + \" Loading model: \" + classifier_names[index])\n",
        "  model = load(model_path)\n",
        "  end_loading_time = dt.now()\n",
        "  print(str(end_loading_time) + \" Finished loading model: \", classifier_names[index])\n",
        "  log.append(str(end_loading_time) + \" Finished loading model: \" + classifier_names[index])\n",
        "  start_time = dt.now()\n",
        "  print(str(start_time) + \" Start calculating AUC ROC using: \", classifier_names[index])\n",
        "  log.append(str(start_time) + \" Start calculating AUC ROC using: \" + classifier_names[index])\n",
        "  auc_scores.append(auc_score(X_T, Y_T, model))\n",
        "  end_time = dt.now()\n",
        "  print(str(end_time) + \" Finished calculating AUC ROC using: \", classifier_names[index])\n",
        "  log.append(str(end_time) + \" Finished calculating AUC ROC using: \" + classifier_names[index])\n",
        "  print(\"Calculation time: \", end_time - start_time)\n",
        "  log.append(\"Calculation time: \" + end_time - start_time)\n",
        "  print(\"--------------------------------------------------------------------------------\")\n",
        "  log(\"/content/drive/My Drive/documents/projects/DeCaf/logs/model_performance.csv\", \\\n",
        "      \"\\n\".join(log))\n",
        "\n",
        "table.add_row(auc_scores)\n",
        "\n",
        "print(table)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}