{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeCaf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue-brQOCOR4A",
        "colab_type": "text"
      },
      "source": [
        "# Notebook for the experiment of building **DeCaf** (**De**sign **C**l**a**ssi**f**ier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_wliyuuasNx",
        "colab_type": "text"
      },
      "source": [
        "## Architectural Overview/Design\n",
        "![alt text](https://raw.githubusercontent.com/alvi2496/DeCaf/master/assets/architectural_overview.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Up9iaDnZM9l",
        "colab_type": "text"
      },
      "source": [
        "## Objective\n",
        "The main objective is to classify discussions from pull request, issue tracker commit messages and code comments as `design` or `general`. We also want to make the classifier cross project compatible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Av-qUhZOah",
        "colab_type": "text"
      },
      "source": [
        "## Data Collection\n",
        "- We intent to have three types of data. One data is to train the `Word Embedding` model. As `Word Embedding` requires structured form of literature, we have used sentences from literatures(ex. papers and books). Also we have restricted our choice of papers and books to only from the Software Engineering domain for keep the context of our `Word Embedding` model restricted to Software Engineering. Our `Word Embedding` model will be used to vectorize our train data.\n",
        "- We have collected our train data from Stack Overflow questions, answers and comments. We have collected data and classified them as `design` or `general` based on the tag. For example, questions and answers that contain `design-patterns` or `software-design` falls under the class of `design` while data tagged as `javascript` or `django` as classified as `general`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2xxEU72ZPf_",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning\n",
        "Raw data can have a lot of noise. Specially when scraped from documents of website, it can contain a lot of misinformation in the form of names, punctuations, numbers(ex. years), misspelled and incompleted words. Also it can have a lot of stopwords that can make the model confused. We have removed all this to make our data as clean as possible. After the cleaning process, out data only contains words that are not stopwords, present in the english dictionary and has lenght greater than three."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A-ie768Gp-2",
        "colab_type": "code",
        "outputId": "af8bfb6a-da1e-44b6-c565-a7a0f7454956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOd6VG5aIqdf",
        "colab_type": "text"
      },
      "source": [
        "## Assign Data location\n",
        "- literature: holds the data for word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9gkg8dEI5fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "literature_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/literature.txt\"\n",
        "enhanced_literature_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/enhanced_literature.txt\"\n",
        "so_model_file = \"/content/drive/My Drive/documents/projects/DeCaf/models/pre_trained/so.bin\"\n",
        "we_model_file = \"/content/drive/My Drive/documents/projects/DeCaf/models/we.bin\"\n",
        "enhanced_we_model_file = \"/content/drive/My Drive/documents/projects/DeCaf/models/enhanced_we.bin\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa6lcx4ePsEM",
        "colab_type": "text"
      },
      "source": [
        "## Enhance the Literature\n",
        "- We enhance the `literature` using pre-trained model `so.bin`\n",
        "  - We are taking every word of literature\n",
        "  - Inject additional similar word related to software engineering to enhance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF2d3MNeblyF",
        "colab_type": "text"
      },
      "source": [
        "### Read the literature.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLcD_v0AbpX_",
        "colab_type": "code",
        "outputId": "38199e8c-272a-4859-d4a3-fa2fda6b8af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "literature = open(literature_file, 'r')\n",
        "words = literature.read().split(\" \")\n",
        "literature.close()\n",
        "print(\"Total words in literature: \", len(words)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in literature:  1575439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FzeUGXLEg-K",
        "colab_type": "text"
      },
      "source": [
        "### Divide the words in chunks\n",
        "- We divide the words in chunks to implement multiprocessing on each chunks\n",
        "- We take 300000 words in each chunks leading to 5 chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Hx6AT9E7XR",
        "colab_type": "code",
        "outputId": "714e703f-4641-458a-f58f-020948136f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "file_chunks = []\n",
        "file_chunks_names = []\n",
        "index = 0\n",
        "while index < len(words) - 1:\n",
        "  chunk = []\n",
        "  lower_limit = index\n",
        "  i = 0\n",
        "  while i < 4 and lower_limit < len(words) - 1:\n",
        "    upper_limit = lower_limit + 75000\n",
        "    if upper_limit > len(words):\n",
        "      upper_limit = len(words) - 1\n",
        "    chunk.append(words[lower_limit:upper_limit])\n",
        "    print(\"lower_limit: \" + str(lower_limit) + \" upper_limit: \" + str(upper_limit))\n",
        "    lower_limit = upper_limit\n",
        "    i = i + 1\n",
        "  file_chunks_names.append(str(index) + '_' + str(lower_limit))\n",
        "  index = lower_limit\n",
        "  file_chunks.append(chunk)\n",
        "print(file_chunks_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lower_limit: 0 upper_limit: 75000\n",
            "lower_limit: 75000 upper_limit: 150000\n",
            "lower_limit: 150000 upper_limit: 225000\n",
            "lower_limit: 225000 upper_limit: 300000\n",
            "lower_limit: 300000 upper_limit: 375000\n",
            "lower_limit: 375000 upper_limit: 450000\n",
            "lower_limit: 450000 upper_limit: 525000\n",
            "lower_limit: 525000 upper_limit: 600000\n",
            "lower_limit: 600000 upper_limit: 675000\n",
            "lower_limit: 675000 upper_limit: 750000\n",
            "lower_limit: 750000 upper_limit: 825000\n",
            "lower_limit: 825000 upper_limit: 900000\n",
            "lower_limit: 900000 upper_limit: 975000\n",
            "lower_limit: 975000 upper_limit: 1050000\n",
            "lower_limit: 1050000 upper_limit: 1125000\n",
            "lower_limit: 1125000 upper_limit: 1200000\n",
            "lower_limit: 1200000 upper_limit: 1275000\n",
            "lower_limit: 1275000 upper_limit: 1350000\n",
            "lower_limit: 1350000 upper_limit: 1425000\n",
            "lower_limit: 1425000 upper_limit: 1500000\n",
            "lower_limit: 1500000 upper_limit: 1575000\n",
            "lower_limit: 1575000 upper_limit: 1575438\n",
            "['0_300000', '300000_600000', '600000_900000', '900000_1200000', '1200000_1500000', '1500000_1575438']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66vRsHcKAWyF",
        "colab_type": "text"
      },
      "source": [
        "### Import the so word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHGyO9D9QZC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime as dt\n",
        "\n",
        "\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "so_model = KeyedVectors.load_word2vec_format(so_model_file, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PvZAqO8k4lj",
        "colab_type": "text"
      },
      "source": [
        "### Inject similar words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JaSUKqccetw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inject_similar_words(process_number, words, injected_word_chunks):\n",
        "  start_time = dt.now()\n",
        "  for i in range(len(words)):\n",
        "    try:\n",
        "      words_to_inject = [words[i]]\n",
        "      similar_words = so_model.most_similar(words[i])\n",
        "      for similar_tuple in similar_words:\n",
        "        if similar_tuple[1] > 0.5:\n",
        "          words_to_inject.append(similar_tuple[0])\n",
        "      words[i] = \" \".join(words_to_inject)\n",
        "    except:\n",
        "      continue\n",
        "    if i % 10000 == 0:\n",
        "      t = dt.now() - start_time\n",
        "      start_time = dt.now()\n",
        "      print(\"CPU \" + str(process_number) + \": Processed: \" + str(i) + \" / \" + str(len(words)) + \" words in time: \", t)\n",
        "  injected_word_chunks[process_number] = words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Oaai2WHZTZ",
        "colab_type": "code",
        "outputId": "bbd01813-4077-41c7-bd95-c9a1af41cafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import multiprocessing as mp\n",
        "\n",
        "print(\"Available CPUs: \", mp.cpu_count())\n",
        "index = 0\n",
        "for word_chunks in file_chunks:\n",
        "  literature_chunk_name = \"/content/drive/My Drive/documents/projects/DeCaf/data/literature_chunks/\" + file_chunks_names[index] + \".txt\"\n",
        "  if not os.path.exists(literature_chunk_name):\n",
        "    print(\"Working on chunk: \", file_chunks_names[index])\n",
        "    manager = mp.Manager()\n",
        "    injected_word_chunks = manager.dict()\n",
        "    jobs = []\n",
        "\n",
        "    for i in range(len(word_chunks)):\n",
        "      p = mp.Process(target=inject_similar_words, args=(i, word_chunks[i], injected_word_chunks))\n",
        "      jobs.append(p)\n",
        "      p.start()\n",
        "\n",
        "    for proc in jobs:\n",
        "      proc.join()\n",
        "\n",
        "    injected_word_chunks = injected_word_chunks.values()\n",
        "\n",
        "    corpus = []\n",
        "    for chunk in injected_word_chunks:\n",
        "      corpus.append(\" \".join(chunk))\n",
        "    corpus = \" \".join(corpus)\n",
        "    literature = open(literature_chunk_name, 'w')\n",
        "    literature.write(corpus)\n",
        "    literature.close()\n",
        "  index = index + 1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available CPUs:  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnuWn5XkCjfO",
        "colab_type": "text"
      },
      "source": [
        "### Concatenate and save the chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QrFDKRdCnOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(enhanced_literature_file):\n",
        "  chunks_directory = \"/content/drive/My Drive/documents/projects/DeCaf/data/literature_chunks/\"\n",
        "  print(chunks_names)\n",
        "  with open(enhanced_literature_file, 'w') as outfile:\n",
        "    for names in chunks_names:\n",
        "      with open(chunks_directory + names) as infile:\n",
        "        outfile.write(infile.read())\n",
        "      outfile.write(\" \")\n",
        "  print('File created!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnD4SBPUJSir",
        "colab_type": "text"
      },
      "source": [
        "## Create Word Embedding\n",
        "- Use fasttext for word embedding\n",
        "  - we take literature.txt as our input data.\n",
        "  - we train the classifier upsupervised since we just want to group the data according to the similarity.\n",
        "  - we have used skipgram as we have observe that skipgram models works better with subword information that cbow\n",
        "  - we are taking words with character number from 4-20. since we are removing every word less than three characters, its not important to take the characters less than 4 characters. Also, the design words seems to be on the bigger side. for ex. reproduceability contains 16 characters. we are considering characters upto 25 characters.\n",
        "  - We are taking 300 dimension of the words. Also looping for 10 epochs. Both because the training corpus is relatively small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8LPXDN4KJMH",
        "colab_type": "code",
        "outputId": "f782455d-c8db-430d-e034-011385a4ac97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2385164 sha256=de8d7a1647e721bed2099191b91292c2a027a7032bc0354dd8d8a4ae1209cb6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZ5xYWTbXoJ",
        "colab_type": "text"
      },
      "source": [
        "## Create Logging Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8H39Gum_Jy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def log(log_file_path, message):\n",
        "  file = pd.DataFrame([[str(dt.now()), 'info', message]], columns=['timestamp', 'type', 'message'])\n",
        "  if not os.path.exists(log_file_path):\n",
        "    file.to_csv(log_file_path)\n",
        "  else:\n",
        "    file.to_csv(log_file_path, mode='a', header=False)\n",
        "\n",
        "def log_result(log_file_path, message):\n",
        "  f = open(log_file_path, \"a\")\n",
        "  f.write(str(dt.now()))\n",
        "  f.write(message)\n",
        "  f.close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9GxG-n5SwdC",
        "colab_type": "text"
      },
      "source": [
        "## Train the `Word Embedding` model and save it as we.bin\n",
        "- Train and save the model if the model is not present\n",
        "- Load the model from disk if preselt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHxShLcsFoa",
        "colab_type": "code",
        "outputId": "898306a8-7d71-41e4-e6af-ea51b93e8ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import fasttext as ft\n",
        "\n",
        "log_file_path = \"/content/drive/My Drive/documents/projects/DeCaf/logs/we.csv\"\n",
        "we_log = []\n",
        "\n",
        "if not os.path.exists(enhanced_we_model_file):\n",
        "  print(str(dt.now())+' Training Word Embedding model...')\n",
        "  we_log.append(str(dt.now())+' Training Word Embedding model...')\n",
        "  we_model = ft.train_unsupervised(enhanced_literature_file, \"skipgram\", minn=4, \\\n",
        "                                   maxn=25, dim=200, epoch=10)\n",
        "  we_model.save_model(enhanced_we_model_file)\n",
        "  print(str(dt.now())+' Model trained and saved successfully')\n",
        "  we_log.append(str(dt.now())+' Model trained and saved successfully')\n",
        "else:\n",
        "  print(str(dt.now())+' Loading Word Embedding model from disk...')\n",
        "  we_log.append(str(dt.now())+' Loading Word Embedding model from disk...')\n",
        "  we_model = ft.load_model(enhanced_we_model_file)\n",
        "  print(str(dt.now())+' Model loaded successfully.')\n",
        "  we_log.append(str(dt.now())+' Model loaded successfully.')\n",
        "\n",
        "log(log_file_path, \\\n",
        "    \"\\n\".join(we_log))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-04 21:58:01.447887 Training Word Embedding model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4ezQOGUS9JO",
        "colab_type": "text"
      },
      "source": [
        "## Playing around with the Word Embedding model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgToIpCwXp5F",
        "colab_type": "text"
      },
      "source": [
        "Get the dimention of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XgUwC1mXsxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_model.get_dimension()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzgaNhGTFeV",
        "colab_type": "text"
      },
      "source": [
        "Get the words of the model... For a demo... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqgszQ6rXBmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_model.get_words()[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfDXAMsiXizi",
        "colab_type": "text"
      },
      "source": [
        "Get the vector of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwQIRXXdXWJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "we_model.get_word_vector('Maintainability')[0:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmzrg38PXzxt",
        "colab_type": "text"
      },
      "source": [
        "Get the vector of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otivEp_7X3I-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vector = we_model.get_sentence_vector(\"Add performance tracker to active admin jobs\")\n",
        "print(vector.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND34XuJDLy2F",
        "colab_type": "text"
      },
      "source": [
        "## Read and process **TRAIN** data\n",
        "- We use three types of data from training the model. We scraped question, answer and comment data from Stack Overflow and tagged them `design` or `general`. The tagging was done automatically based on the original tag of the question. Then we process our data as we did before for our word embedding data to clear noise. We also did some additional processing to our train data. We only took those documents that contains more than 10 words. For the others, we discarded them. After processing, we got 1,00,000 documents(50,000-design, 50,000-general) for `questions.csv`, 40,000 documents(20,000-design, 20,000-general) for `answers.csv` and 60,000 documents(30,000-design, 30,000-general) for `comments.csv`.\n",
        "\n",
        "- Our train data is completely noise free, stopwords free and long documents of more than 10 words per document. All the data is randomly distributed.\n",
        "\n",
        "- After processing the data and converting the data in vector with the help of our trained word embedding model, we save the data as .npy file for quick access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E30pMihMDCA",
        "colab_type": "text"
      },
      "source": [
        "#### Assign train data location\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGy3g1XTOC2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_question_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/questions.csv\"\n",
        "train_answer_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/answers.csv\"\n",
        "train_comment_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/comments.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjduBEkLOjcN",
        "colab_type": "text"
      },
      "source": [
        "#### Read the data with pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw_-kkRROo5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "train_question = pd.read_csv(train_question_file)\n",
        "train_answer = pd.read_csv(train_answer_file)\n",
        "train_comment = pd.read_csv(train_comment_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NazCrfAzPD0J",
        "colab_type": "text"
      },
      "source": [
        "#### Explore the train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbwPwlRvT2oJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Question train data\")\n",
        "print(train_question.head())\n",
        "print(\"Answer train data\")\n",
        "print(train_answer.head())\n",
        "print(\"Comment train data\")\n",
        "print(train_comment.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZR0EwyIPOvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = [\"Dataset Name\", \"Shape\", \"# of design data\", \"# of general data\"]\n",
        "\n",
        "qd = train_question[train_question['label']=='design']\n",
        "qg = train_question[train_question['label']=='general']\n",
        "\n",
        "ad = train_answer[train_answer['label']=='design']\n",
        "ag = train_answer[train_answer['label']=='general']\n",
        "\n",
        "cd = train_comment[train_comment['label']=='design']\n",
        "cg = train_comment[train_comment['label']=='general']\n",
        "\n",
        "table.add_row([\"Question\", train_question.shape, qd.shape[0], qg.shape[0]])\n",
        "table.add_row([\"Answer\", train_answer.shape, ad.shape[0], ag.shape[0]])\n",
        "table.add_row([\"Comment\", train_comment.shape, cd.shape[0], cg.shape[0]])\n",
        "\n",
        "\n",
        "print(table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ntcn6aKPE3",
        "colab_type": "text"
      },
      "source": [
        "### Convert sentences in vector uisng Word Embedding model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q52NSKvjeoQA",
        "colab_type": "text"
      },
      "source": [
        "Now we turn our head towards converting our train data to vectors of 300 dimension. We are naming our variable by the following convension:\n",
        "- question data = X_Q, question label = Y_Q\n",
        "- answer data = X_A, answer label = Y_A\n",
        "- comment data = X_C, comment label = Y_C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TmaOl6StWKM",
        "colab_type": "text"
      },
      "source": [
        "#### Location to save/retrive data in vector format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz5I9xV8tkEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Q_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/X_Q.npy\"\n",
        "Y_Q_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/Y_Q.npy\"\n",
        "\n",
        "X_A_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/X_A.npy\"\n",
        "Y_A_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/Y_A.npy\"\n",
        "\n",
        "X_C_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/X_C.npy\"\n",
        "Y_C_url = \"/content/drive/My Drive/documents/projects/DeCaf/data/train_data/Y_C.npy\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q47D7-6yVIr",
        "colab_type": "text"
      },
      "source": [
        "We define `get_text_vector(data, url)` function to convert text sentences into 300 dimension word vector. We define `get_label_vector(label, url)` to convert the labels into vector.\n",
        "- look for the .npy file in the disk. if found then load the data into memory.\n",
        "- if not found then \n",
        "  - convert the text and labels into vector\n",
        "  - save the text and labels as .npy file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDBnIkvafAV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_text_vector(data, data_url):\n",
        "  if os.path.exists(data_url):\n",
        "    X = np.load(data_url)\n",
        "  else:\n",
        "    X = []\n",
        "    for sentence in data:\n",
        "      X.append(we_model.get_sentence_vector(sentence))\n",
        "    X = np.array(X)\n",
        "    np.save(data_url, X)\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpb4wbAJgYrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label_vector(labels, label_url):\n",
        "  if os.path.exists(label_url):\n",
        "    Y = np.load(label_url)\n",
        "  else:\n",
        "    Y = []\n",
        "    for label in labels:\n",
        "      if label == 'design':\n",
        "        Y.append(1)\n",
        "      else:\n",
        "        Y.append(0)\n",
        "    Y = np.array(Y)\n",
        "    np.save(label_url, Y)\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-7wCNmayK9i",
        "colab_type": "text"
      },
      "source": [
        "#### Load and inspect the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lo1ihPqbyCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_Q = get_text_vector(train_question['question'], X_Q_url)\n",
        "Y_Q = get_label_vector(train_question['label'], Y_Q_url)\n",
        "print('Shape of X_Q: ', X_Q.shape)\n",
        "print('Shape of Y_Q: ', Y_Q.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NjL4tUcdb8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_A = get_text_vector(train_answer['answer'], X_A_url)\n",
        "Y_A = get_label_vector(train_answer['label'], Y_A_url)\n",
        "print('Shape of X_A: ', X_A.shape)\n",
        "print('Shape of Y_A: ', Y_A.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2-zNJMnspD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_C = get_text_vector(train_comment['comment'], X_C_url)\n",
        "Y_C = get_label_vector(train_comment['label'], Y_C_url)\n",
        "print('Shape of X_C: ', X_C.shape)\n",
        "print('Shape of Y_C: ', Y_C.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdvw7gNIVSEf",
        "colab_type": "text"
      },
      "source": [
        "## Read and process **VALIDATION** data\n",
        "- We are taking the same approach as above to read, process, convert and save our validation data.\n",
        "- Validation data contains a mixture of `question`, `answer` and `comment` data\n",
        "- X_V for the vector of the text, Y_V is the vector of the labels for validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNeKSdPtV9Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/validation_data/validation.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y0abI0aWMCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = pd.read_csv(validation_data_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l240dbRLWXGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Shape: \", validation_data.shape)\n",
        "print(validation_data.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuZkcU_aWn5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of design data: \", validation_data[validation_data[\"label\"] == \"design\"].shape[0])\n",
        "print(\"number of general data: \", validation_data[validation_data[\"label\"] == \"general\"].shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmQ45P1QXOnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_V = get_text_vector(validation_data['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/validation_data/X_V.npy\")\n",
        "Y_V = get_label_vector(validation_data['label'], \"/content/drive/My Drive/documents/projects/DeCaf/data/validation_data/Y_V.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grhes7JkX2ql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Shape of X_V: \", X_V.shape)\n",
        "print(\"Shape of Y_V: \", Y_V.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv7WbsJpkquT",
        "colab_type": "text"
      },
      "source": [
        "## Read and process **TEST** data\n",
        "- We are taking the same approach as above to read, process, convert and save our validation data.\n",
        "- Test data contains a mixture of `question`, `answer` and `comment` data\n",
        "- X_T for the vector of the text, Y_T is the vector of the labels for validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "293UpZ0SkzhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_file = \"/content/drive/My Drive/documents/projects/DeCaf/data/test_data/test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qddbU7FHk9MK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(test_data_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuJEcM1DlFVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Shape: \", test_data.shape)\n",
        "print(test_data.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S-jB0F9lNaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of design data: \", test_data[test_data[\"label\"] == \"design\"].shape[0])\n",
        "print(\"number of general data: \", test_data[test_data[\"label\"] == \"general\"].shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHrhJIDolUO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_T = get_text_vector(test_data['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/test_data/X_T.npy\")\n",
        "Y_T = get_label_vector(test_data['label'], \"/content/drive/My Drive/documents/projects/DeCaf/data/test_data/Y_T.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y4OAmiklsIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Shape of X_T: \", X_T.shape)\n",
        "print(\"Shape of Y_T: \", Y_T.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyL6iWCiNlwR",
        "colab_type": "text"
      },
      "source": [
        "## Experiment with Traditional Data Mining Algorithms\n",
        "- Before start experimenting with deep learning, we start our experiment with training some traditional data mining algorithms. We are taking the following classifiers with notations and parameter configurations:\n",
        "  - k-Nearest Neighbors (`knn`)\n",
        "  - Decision Tree (`dt`)\n",
        "  - Random Forest (`rf`)\n",
        "  - Logistic Regression (`lr`)\n",
        "  - Linear SVM (`lsvm`)\n",
        "    - C = Regularization Parameter\n",
        "  - RBF SVM (`rbf_svm`)\n",
        "    - Kernel coefficient = 2 \n",
        "    - Regularization parameter = default = 1\n",
        "  - Neural Net (`nn`)\n",
        "  - AdaBoost (`ab`)\n",
        "  - Naive Bayes (`gnb`)\n",
        "  - QDA (`qda`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-7yQzbVCW7",
        "colab_type": "text"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KML4RtuuRum4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq7d0YOLVF6K",
        "colab_type": "text"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsab_DKqQAak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "classifier_names = [\n",
        "      \"Nearest Neighbors\",\n",
        "      \"Decision Tree\",\n",
        "      \"Random Forest\",\n",
        "      \"Logistic Regression\",\n",
        "      \"Gaussian Naive Bayes\", \n",
        "      \"Neural Net\", \n",
        "      \"AdaBoost\",\n",
        "      \"QDA\",    \n",
        "      \"Linear SVM\", \n",
        "      \"RBF SVM\",\n",
        "]\n",
        "\n",
        "model_paths = [\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/knn.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/dt.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/rf.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/lr.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/gnb.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/nn.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/ab.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/qda.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/lsvm.joblib\",\n",
        "      \"/content/drive/My Drive/documents/projects/DeCaf/models/rbf_svm.joblib\"\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "      KNeighborsClassifier(n_jobs=-1),\n",
        "      DecisionTreeClassifier(), \n",
        "      RandomForestClassifier(n_jobs=-1), \n",
        "      LogisticRegression(max_iter=50000),\n",
        "      GaussianNB(),\n",
        "      MLPClassifier(max_iter=4000),\n",
        "      AdaBoostClassifier(),\n",
        "      QuadraticDiscriminantAnalysis(),\n",
        "      SVC(kernel=\"linear\", C=0.025), \n",
        "      SVC(gamma=2, C=1)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Q0fquUXt8c",
        "colab_type": "text"
      },
      "source": [
        "### Combine the three train data namely: `questions`, `answers` and `comment`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj2e6MSCX9Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.concatenate((X_Q, X_A, X_C), axis=0)\n",
        "Y = np.concatenate((Y_Q, Y_A, Y_C), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TozZfIXeYaw2",
        "colab_type": "text"
      },
      "source": [
        "### Examine X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbxIWifrYfYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kV5XMI7PYOP",
        "colab_type": "text"
      },
      "source": [
        "### **Train** and **Save** the models into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQswNls3VuOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "logs = []\n",
        "\n",
        "for index, classifier in enumerate(classifiers):\n",
        "  if not os.path.exists(model_paths[index]):\n",
        "    start_time = dt.now()\n",
        "    print(str(start_time) + \" Started training model: \", classifier_names[index])\n",
        "    logs.append(str(start_time) + \" Started training model: \", classifier_names[index])\n",
        "    model = classifier.fit(X, Y)\n",
        "    end_time = dt.now()\n",
        "    print(str(end_time) + \" Finished training model: \", classifier_names[index])\n",
        "    logs.append(str(end_time) + \" Finished training model: \", classifier_names[index])\n",
        "    print(\"Time to train model: \", end_time - start_time)\n",
        "    logs.append(\"Time to train model: \", end_time - start_time)\n",
        "    print(\"----------------------------------------------------------\")\n",
        "    dump(model, model_paths[index])\n",
        "    log(\"/content/drive/My Drive/documents/projects/DeCaf/logs/model_train.csv\", \"\\n\".join(log))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljPxEs-4Ngx0",
        "colab_type": "text"
      },
      "source": [
        "### Validate the models\n",
        "- We use Area Under the Receiver Operating Characteristic Curve (**ROC AUC**) from prediction scores as the validation criteria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLxh9uyZN4Kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def auc_score(X, Y, model):\n",
        "  pred = model.predict(X)\n",
        "  auc = metrics.roc_auc_score(Y, pred)\n",
        "  return auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQKN9WbuOIus",
        "colab_type": "text"
      },
      "source": [
        "#### Load the models and calculate the **ROC AUC** score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Fj_9TWOREB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_path = \"/content/drive/My Drive/documents/projects/DeCaf/results/test_data.txt\"\n",
        "\n",
        "if not os.path.exists(result_path):\n",
        "  table = PrettyTable()\n",
        "\n",
        "  table.field_names = classifier_names\n",
        "  log = []\n",
        "  auc_scores = []\n",
        "  for index, model_path in enumerate(model_paths):\n",
        "    start_loading_time = dt.now()\n",
        "    print(str(start_loading_time) + \" Loading model: \", classifier_names[index])\n",
        "    log.append(str(start_loading_time) + \" Loading model: \" + classifier_names[index])\n",
        "    model = load(model_path)\n",
        "    end_loading_time = dt.now()\n",
        "    print(str(end_loading_time) + \" Finished loading model: \", classifier_names[index])\n",
        "    log.append(str(end_loading_time) + \" Finished loading model: \" + classifier_names[index])\n",
        "    start_time = dt.now()\n",
        "    print(str(start_time) + \" Start calculating AUC ROC using: \", classifier_names[index])\n",
        "    log.append(str(start_time) + \" Start calculating AUC ROC using: \" + classifier_names[index])\n",
        "    auc_scores.append(auc_score(X_T, Y_T, model))\n",
        "    end_time = dt.now()\n",
        "    print(str(end_time) + \" Finished calculating AUC ROC using: \", classifier_names[index])\n",
        "    log.append(str(end_time) + \" Finished calculating AUC ROC using: \" + classifier_names[index])\n",
        "    print(\"Calculation time: \", end_time - start_time)\n",
        "    log.append(\"Calculation time: \" + end_time - start_time)\n",
        "    print(\"--------------------------------------------------------------------------------\")\n",
        "    log(\"/content/drive/My Drive/documents/projects/DeCaf/logs/model_performance.csv\", \\\n",
        "        \"\\n\".join(log))\n",
        "\n",
        "  table.add_row(auc_scores)\n",
        "\n",
        "  print(table)\n",
        "  log_result(result_path, table.get_string())\n",
        "else:\n",
        "  print(\"Result persists at /content/drive/My Drive/documents/projects/DeCaf/results/test_data.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLFv8r8enaY-",
        "colab_type": "text"
      },
      "source": [
        "## Cross dataset validation of the models\n",
        "We are taking the following datasets to validate the models:\n",
        "- Brunet 2014 (brunet2014.csv)\n",
        "- Shakiba 2016 (shakiba2016.csv)\n",
        "- Viviani 2018 (viviani2018.csv)\n",
        "- Self Admitted Technical Debt/ SATD (satd.csv)\n",
        "- Stack Overflow (so.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt9XbknhoMh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_dataset_names = [\n",
        "                       \"Brunet 2014\", \"Shakiba 2016\", \"Viviani 2018\",\n",
        "                       \"SATD\"\n",
        "]\n",
        "\n",
        "brunet2014 = pd.read_csv(\"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/brunet2014.csv\")\n",
        "shakiba2016 = pd.read_csv(\"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/shakiba2016.csv\")\n",
        "viviani2018 = pd.read_csv(\"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/viviani2018.csv\")\n",
        "satd = pd.read_csv(\"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/satd.csv\")\n",
        "so = pd.read_csv(\"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/so.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P9-UeiAwzww",
        "colab_type": "text"
      },
      "source": [
        "### Examine the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJiihXYew_zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = [\"Dataset Name\", \"Total # of rows\", \"# of design\", \"# of general\"]\n",
        "table.add_row([\"Brunet 2014\", brunet2014.shape[0], \\\n",
        "               brunet2014[brunet2014['label'] == 1].shape[0], \\\n",
        "               brunet2014[brunet2014['label'] == 0].shape[0]])\n",
        "table.add_row([\"Shakiba 2016\", shakiba2016.shape[0], \\\n",
        "               shakiba2016[shakiba2016['label'] == 1].shape[0], \\\n",
        "               shakiba2016[shakiba2016['label'] == 0 ].shape[0]])\n",
        "table.add_row([\"Viviani 2018\", viviani2018.shape[0], \\\n",
        "               viviani2018[viviani2018['label'] == 1].shape[0], \\\n",
        "               viviani2018[viviani2018['label'] == 0 ].shape[0]])\n",
        "table.add_row([\"SATD\", satd.shape[0], \\\n",
        "               satd[satd['label'] == 1].shape[0], \\\n",
        "               satd[satd['label'] == 0 ].shape[0]])\n",
        "table.add_row([\"Stack overflow\", so.shape[0], \\\n",
        "               so[so['label'] == 1].shape[0], \\\n",
        "               so[so['label'] == 0 ].shape[0]])\n",
        "print(table)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L24EjgKXpJY1",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize and Save the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCM9TJfzpZeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_brunet2014 = get_text_vector(brunet2014['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/X_brunet2014.npy\")\n",
        "Y_brunet2014 = brunet2014['label']\n",
        "print(X_brunet2014.shape)\n",
        "print(Y_brunet2014.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXLiNFapsIbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_shakiba2016 = get_text_vector(shakiba2016['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/X_shakiba2016.npy\")\n",
        "Y_shakiba2016 = shakiba2016['label']\n",
        "print(X_shakiba2016.shape)\n",
        "print(Y_shakiba2016.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKuvSBTGsJG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_viviani2018 = get_text_vector(viviani2018['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/X_viviani2018.npy\")\n",
        "Y_viviani2018 = viviani2018['label']\n",
        "print(X_viviani2018.shape)\n",
        "print(Y_viviani2018.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJvSgex_sJgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_satd = get_text_vector(satd['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/X_satd.npy\")\n",
        "Y_satd = satd['label']\n",
        "print(X_satd.shape)\n",
        "print(Y_satd.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgaq6VXPsJ-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_so = get_text_vector(so['text'], \"/content/drive/My Drive/documents/projects/DeCaf/data/cross_data/processed/X_so.npy\")\n",
        "Y_so = so['label']\n",
        "print(X_so.shape)\n",
        "print(Y_so.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsuSsmFCtf2J",
        "colab_type": "text"
      },
      "source": [
        "### Validate with the Trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aWuwFIZtlLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = [X_brunet2014, X_shakiba2016, X_viviani2018, X_satd]\n",
        "test_label = [Y_brunet2014, Y_shakiba2016, Y_viviani2018, Y_satd]\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = [\" \"] + cross_dataset_names\n",
        "\n",
        "for index, model_path in enumerate(model_paths):\n",
        "  logs = []\n",
        "  print(str(dt.now()) + \" Start loading model: \", classifier_names[index])\n",
        "  logs.append(str(dt.now()) + \" Start loading model: \" + classifier_names[index])\n",
        "  model = load(model_path)\n",
        "  row = [classifier_names[index]]\n",
        "  for i, data in enumerate(test_data):\n",
        "    start_time = dt.now()\n",
        "    print(str(start_time) + \" Start evaluating \", cross_dataset_names[i])\n",
        "    row.append(auc_score(data, test_label[i], model))\n",
        "    end_time = dt.now()\n",
        "    print(str(end_time) + \" Finished evaluating \", cross_dataset_names[i])\n",
        "    total_time = end_time - start_time\n",
        "    logs.append(\"Evaluation time: \" + str(total_time))\n",
        "  log(\"/content/drive/My Drive/documents/projects/DeCaf/logs/cross_data_evaluation.csv\", \\\n",
        "      \"\\n\".join(logs))\n",
        "  table.add_row(row)\n",
        "\n",
        "print(table)\n",
        "log_result(\"/content/drive/My Drive/documents/projects/DeCaf/results/cross_data.txt\", table.get_string())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}